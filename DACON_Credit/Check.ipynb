{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Check.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"15hRMKr2ZP8Vcb-MJIqxc2Zo5oDxE1LWC","authorship_tag":"ABX9TyOB8LFZGRDkLgz68IFsza1U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"MOI4O-_sTcbU"},"source":["!pip install scikit-learn==0.22.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzpEuxXKTbpZ"},"source":["!pip show imbalanced-learn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vgQkNKiQItnc"},"source":["pip install kaggler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rm36k9T0Ipgu"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd\n","import numpy as np\n","import random\n","import glob\n","\n","import lightgbm as lgb\n","from lightgbm import LGBMClassifier\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.metrics import log_loss\n","\n","from kaggler.model import AutoLGB\n","from kaggler.preprocessing import LabelEncoder\n","\n","from pprint import pprint\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.datasets import make_classification\n","from sklearn.decomposition import PCA\n","import six\n","import sys\n","sys.modules['sklearn.externals.six'] = six\n","from sklearn.externals import six\n","from imblearn.over_sampling import SMOTE\n","\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEPw_FOiycBB"},"source":["CV Log Loss: 0.750443\n","2    0.8969\n","1    0.0906\n","0    0.0125"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGm6OmpYIqrA"},"source":["def days_to_age(x) :\n","    return int(((x*-1)/365)+1)\n","    \n","def months_to_year(x) :\n","    return int(((x*-1)/12)+1)\n","\n","def days_to_month(x) :\n","    return int(((x*-1)/30)+1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TC0ek73VIvvs"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDporoor6ZgC"},"source":["PATH = './drive/MyDrive/Credit/data/'\n","train = pd.read_csv(PATH+'train.csv')\n","test = pd.read_csv(PATH+'test.csv')\n","submit = pd.read_csv(PATH+'sample_submission.csv')\n","\n","train.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xDrUrkaNEWEp"},"source":["* Credit외 완전 중복 상태의 데이터가 3155개 존재\n","    * 다행히 완전 중복의 경우 credit이 변하지 않았음"]},{"cell_type":"code","metadata":{"id":"E9EPYj5t4eNS"},"source":["# 완전 중복 확인\n","train['iden'] = train.apply(lambda x : \"\".join([str(x[i]) for i in list(set(train.columns)-set('credit'))]), axis=1)\n","\n","# for i in range(len(train['iden'].value_counts()[train['iden'].value_counts()>1].index)):\n","#     if train[train['iden'] == train['iden'].value_counts()[train['iden'].value_counts()>1].index[i]]['credit'].nunique() != 1:\n","#         print(i)\n","\n","dupli_df = train.loc[train['iden'].isin(train['iden'].value_counts()[train['iden'].value_counts()>1].index)]\n","drop_list = list(set(dupli_df.index) - set(dupli_df.drop_duplicates().index))\n","\n","# 중복 데이터 중 첫 번째 데이터 제외 삭제\n","train = train[~train.index.isin(drop_list)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N2yKPfNu8DIR"},"source":["* Credit외 완전 중복 상태의 데이터가 3155개 존재\n","    * 다행히 완전 중복의 경우 credit이 변하지 않았음\n","\n","* 내 생각에는 income_type이 유의미하지 않은 변수인 듯"]},{"cell_type":"code","metadata":{"id":"oktlBHFr9G4z"},"source":["train['credit'].value_counts() / train['credit'].count(),\\\n","train.loc[train['income_type']=='State servant']['credit'].value_counts() / train.loc[train['income_type']=='State servant']['credit'].count(),\n","train['income_type'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYyokxfn6bSV"},"source":["train['identity'] = [str(i)+str(j)+str(k)+str(l)+str(m) for i,j,k,l,m in zip(train['gender'], train['DAYS_BIRTH'], train['DAYS_EMPLOYED'], train['edu_type'], train['income_total'])]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qWx5nTjW7NTG"},"source":["dupli_col = train['identity'].value_counts().index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VUeDgkF-7d50"},"source":["surv_list = []\n","for i in range(len(dupli_col)) :\n","    surv_list.append(train.loc[train['identity']==dupli_col[i]].sort_values(['begin_month']).drop_duplicates('credit',keep='first').index[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOj0B-NQ8l3T"},"source":["train = train[train.index.isin(surv_list)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hb97MvCR9iWk"},"source":["test.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n","\n","train.loc[train['DAYS_EMPLOYED']>0, 'DAYS_EMPLOYED'] = 0\n","test.loc[test['DAYS_EMPLOYED']>0, 'DAYS_EMPLOYED'] = 0\n","\n","# occyp_type을 쓸거냐 안쓸거냐의 차이 / 없을 때가 성능이 더 뛰어났긴 함\n","train.loc[train['DAYS_EMPLOYED'] == 0, 'occyp_type'] = 'Unemployed'\n","test.loc[test['DAYS_EMPLOYED'] == 0, 'occyp_type'] = 'Unemployed'\n","train.dropna(inplace=True)\n","\n","train = train[train['child_num'] < train['family_size']]\n","\n","\n","\n","\n","\n","\n","train.reset_index(drop=True, inplace=True)\n","\n","df = pd.concat([train.drop('credit', axis=1), test])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ytU-E3P5ykfN"},"source":["PATH = './drive/MyDrive/Credit/data/'\n","train = pd.read_csv(PATH+'train.csv')\n","test = pd.read_csv(PATH+'test.csv')\n","submit = pd.read_csv(PATH+'sample_submission.csv')\n","\n","\n","\n","train.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n","test.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n","\n","train.loc[train['DAYS_EMPLOYED']>0, 'DAYS_EMPLOYED'] = 0\n","test.loc[test['DAYS_EMPLOYED']>0, 'DAYS_EMPLOYED'] = 0\n","\n","# occyp_type을 쓸거냐 안쓸거냐의 차이 / 없을 때가 성능이 더 뛰어났긴 함\n","train.loc[train['DAYS_EMPLOYED'] == 0, 'occyp_type'] = 'Unemployed'\n","test.loc[test['DAYS_EMPLOYED'] == 0, 'occyp_type'] = 'Unemployed'\n","train.dropna(inplace=True)\n","\n","train = train[train['child_num'] < train['family_size']]\n","\n","\n","\n","\n","\n","\n","train.reset_index(drop=True, inplace=True)\n","\n","df = pd.concat([train.drop('credit', axis=1), test])\n","# df.dropna(inplace=True)\n","# df.fillna('Unknown', inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Pj7KCRqfrd8"},"source":["CV Log Loss: 0.736840\n","2    0.8693\n","1    0.1058\n","0    0.0249"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQ9AOO7qs5mp"},"source":["df['age_gen'] = df['DAYS_BIRTH'].apply(lambda x : '60' if days_to_age(x)//10*10 >= 60 else ( '10' if days_to_age(x)//10*10 < 20 else str(days_to_age(x)//10*10)))+ '_' + df['gender']\n","df['phone_mail'] = df.apply(lambda x : str(x['phone'])+'_'+str(x['email'])+'_'+str(x['work_phone']), axis=1)\n","df['real_car'] = df.apply(lambda x : str(x['reality'])+'_'+str(x['car']), axis=1)\n","\n","df.drop(['gender', 'email', 'phone', 'car','reality', 'work_phone'], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ctc9CPMuynAd"},"source":["# test['identity'] = [str(i)+str(j)+str(k)+str(l)+str(m) for i,j,k,l,m in zip(test['gender'], test['DAYS_BIRTH'], test['DAYS_EMPLOYED'], test['edu_type'], test['income_total'])]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3UT-5C8I1XQ"},"source":["# df.loc[df['DAYS_EMPLOYED']>=0, 'DAYS_EMPLOYED'] = 0\n","# df.loc[df['DAYS_EMPLOYED']==0, 'occyp_type'] = 'Unemployed'\n","\n","df['before_EMPLOYED']=df['DAYS_BIRTH']-df['DAYS_EMPLOYED']\n","df['before_EMPLOYED_month']=np.floor((-df['before_EMPLOYED'])/30)-((np.floor((-df['before_EMPLOYED'])/30)/12).astype(int)*12)\n","df['before_EMPLOYED_week']=np.floor((-df['before_EMPLOYED'])/7)-((np.floor((-df['before_EMPLOYED'])/7)/4).astype(int)*4)\n","\n","df['DAYS_BIRTH_month']=np.floor((-df['DAYS_BIRTH'])/30)-((np.floor((-df['DAYS_BIRTH'])/30)/12).astype(int)*12)\n","df['DAYS_BIRTH_week']=np.floor((-df['DAYS_BIRTH'])/7)-((np.floor((-df['DAYS_BIRTH'])/7)/4).astype(int)*4)\n","\n","df['DAYS_EMPLOYED_month']=np.floor((-df['DAYS_EMPLOYED'])/30)-((np.floor((-df['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n","df['DAYS_EMPLOYED_week']=np.floor((-df['DAYS_EMPLOYED'])/7)-((np.floor((-df['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n","\n","df['DAYS_BIRTH'] = -df['DAYS_BIRTH']\n","df['DAYS_EMPLOYED'] = -df['DAYS_EMPLOYED']\n","df['begin_month'] = -df['begin_month']\n","df['before_EMPLOYED'] = - df['before_EMPLOYED']\n","\n","df['begin_credit'] = df['DAYS_EMPLOYED'] - 30*df['begin_month']\n","# df['BEFORE_EMPLOYED'] = df['BEFORE_EMPLOYED'].apply(lambda x : days_to_age(x))\n","# df['DAYS_BIRTH'] = df['DAYS_BIRTH'].apply(lambda x : days_to_age(x))\n","# df['DAYS_EMPLOYED'] = df['DAYS_EMPLOYED'].apply(lambda x : days_to_month(x))\n","\n","# df[['work_phone', 'phone', 'email']] = df[['work_phone', 'phone', 'email']].astype('object')\n","\n","edu_dict = {'Academic degree' : 4, # 학사 이상\n","            'Higher education' : 3, # 고등학교 졸업\n","            'Incomplete higher' : 2, # 고등학교 중퇴\n","           'Secondary / secondary special' : 1, # 중학교\n","            'Lower secondary' : 0} # 중학교 미만\n","\n","df['edu_type'] = df['edu_type'].apply(lambda x: edu_dict[x])\n","\n","# df['temp'] = df['family_size'] - df['child_num']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JfGM0NqvbxJO"},"source":["이번에는, Family Type를 확인해보자.\n","Married의 비중이 압도적으로 많다.\n","카드사 데이터 특징 중, 실질적인 카드 소유권이 남성보다는 여성이 많고, 미혼보다는 기혼이 더 많다는 것을 확인할 수 있다.\n","\n","이번에는, Education Type를 확인해보자.\n","Credit 2번 그룹을 중심으로 Secondary / Secondary Special 그룹과 Higher education 그룹에서 많은 빈도가 나타나고 있다."]},{"cell_type":"code","metadata":{"id":"qUpmj8BEI712"},"source":["ob_col = [x for x in df.columns if df[x].dtype == 'object']\n","# label_col = ob_col[:3] + ['age_gen']\n","# ordinal_col = ['income_type', 'family_type', 'house_type', 'occyp_type']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hfQrFh0KxdQ"},"source":["lbe = LabelEncoder(min_obs=10)\n","df[ob_col] = lbe.fit_transform(df[ob_col])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jk9TOEnS3Kk-"},"source":["ordinal_col = ['age_gen', 'phone_mail', 'real_car']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5D1jnTvUD_i"},"source":["# Binarize\n","import numpy as np\n","from scipy import sparse\n","\n","def binarize_target(target_col):\n","    n = df[target_col].nunique()\n","    print('number of cateories : ', n)\n","    binarize_labels = sorted(df[target_col].unique())\n","    print('binarize target labels : ', binarize_labels)\n","    k = 0\n","    while n > 2**k:\n","        k += 1\n","    print('necessary number for binarize : ', k)\n","\n","    binarized_result = np.zeros((df.shape[0],k))\n","\n","    binarized_values = df[target_col].apply(lambda x : list(map(int,bin(x).replace('0b','').zfill(k)))).values\n","    for index, value in enumerate(binarized_values):\n","        binarized_result[index] = np.array(value)\n","\n","    binarized_result = pd.DataFrame(binarized_result, columns = [target_col+'_'+str(x) for x in range(k)],\n","                                    index=df.index)\n","    return binarized_result\n","\n","for ordinal in ordinal_col : \n","    binar_df = binarize_target(ordinal)\n","    df = pd.concat([df, binar_df], axis=1).drop(columns=[ordinal])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMmbCn2cUJYQ"},"source":["import scipy.stats as ss\n","\n","normalize_list = ['income_total', 'DAYS_BIRTH', 'DAYS_EMPLOYED'] + ['begin_month', 'before_EMPLOYED', 'before_EMPLOYED_month', 'before_EMPLOYED_week', 'DAYS_BIRTH_month', 'DAYS_BIRTH_week', 'DAYS_EMPLOYED_month', 'DAYS_EMPLOYED_week', 'begin_credit']\n","normal_df = pd.DataFrame(ss.zscore(df[normalize_list]), columns=normalize_list, index=df.index)\n","df = pd.concat([df.drop(columns=normalize_list, axis=1), normal_df], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JmFs0xSDXMrY"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uni229meKCKK"},"source":["y = train['credit']\n","n_trn = train.shape[0]\n","X = df.iloc[:n_trn]\n","X_tst = df.iloc[n_trn:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NO7vdtYbJ9io"},"source":["# scaler = MinMaxScaler(feature_range=(0, 1))\n","# # scaler.fit_transform(X)\n","# X = scaler.fit_transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPhGysd2Mwhx"},"source":["sm = SMOTE(ratio = 'auto', kind='regular')\n","\n","X_resampled, y_resampled = sm.fit_sample(X, list(y))\n","\n","print('After OverSampling, the shape of train_X: {}'.format(X_resampled.shape))\n","print('After OverSampling, the shape of train_y: {} \\n'.format(X_resampled.shape))\n","\n","print(\"After OverSampling, counts of label '1': {}\".format(sum(y_resampled==1)))\n","print(\"After OverSampling, counts of label '0': {}\".format(sum(y_resampled==0)))\n","print(\"After OverSampling, counts of label '2': {}\".format(sum(y_resampled==2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Tt97HhUSOKP"},"source":["X = pd.DataFrame(X_resampled, columns=X.columns)\n","X['credit'] = y_resampled"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IerW4QZLhaHL"},"source":["# X = pd.DataFrame(X, columns=df.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XKFHWHNXJE67"},"source":["y = X['credit']\n","X = X.drop(['credit'], axis=1)\n","# n_trn = train.shape[0]\n","# # X = df.iloc[:n_trn]\n","# X_tst = df.iloc[n_trn:]\n","\n","n_class = 3\n","n_fold = 5\n","\n","\n","cv = StratifiedKFold(n_splits=n_fold)\n","\n","p = np.zeros((X.shape[0], n_class), dtype=float)\n","p_tst = np.zeros((X_tst.shape[0], n_class), dtype=float)\n","# params = {'num_class': n_class}\n","params = {'bagging_fraction': 0.7000000000000001, 'bagging_freq': 1, 'boosting': 'gbdt', 'feature_fraction': 0.5,\\\n","          'feature_pre_filter': False, 'lambda_l1': 0, 'lambda_l2': 0.1, 'learning_rate': 0.020391961848274073,\\\n","          'max_depth': -1, 'metric': 'multi_logloss', 'min_child_samples': 10, 'num_class': 3, 'num_leaves': 255,\\\n","          'num_threads': -1, 'objective': 'multiclass', 'seed': 42, 'verbosity': -1}\n","# features = ['gender', 'car', 'reality', 'child_num', 'income_total', 'edu_type', 'DAYS_BIRTH', 'DAYS_EMPLOYED',\\\n","#             'work_phone', 'phone', 'email', 'family_size', 'begin_month', 'before_EMPLOYED', 'before_EMPLOYED_month',\\\n","#             'before_EMPLOYED_week', 'DAYS_BIRTH_month', 'DAYS_BIRTH_week', 'DAYS_EMPLOYED_month', 'DAYS_EMPLOYED_week',\\\n","#             'income_type_0', 'income_type_1', 'income_type_2', 'family_type_0', 'family_type_1', 'family_type_2',\\\n","#             'house_type_0', 'house_type_1', 'house_type_2', 'occyp_type_0', 'occyp_type_1', 'occyp_type_2', 'occyp_type_3', 'occyp_type_4']\n","features = df.columns\n","n_best = 206\n","\n","# params = {'bagging_freq': 1, 'verbosity': -1, 'seed': 42, 'num_threads': -1, 'feature_pre_filter': False, 'num_class': 3, 'objective': 'multiclass', 'metric': 'multi_logloss', 'boosting': 'gbdt', 'bagging_fraction': 0.9, 'feature_fraction': 0.7000000000000001, 'lambda_l1': 0, 'lambda_l2': 0.1, 'learning_rate': 0.23323579910038686, 'max_depth': 4, 'min_child_samples': 25, 'num_leaves': 15}\n","# features = ['gender', 'car', 'reality', 'child_num', 'income_total', 'income_type', 'edu_type', 'family_type', 'house_type', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'work_phone', 'phone', 'email', 'occyp_type', 'family_size', 'begin_month', 'before_EMPLOYED', 'before_EMPLOYED_month', 'before_EMPLOYED_week', 'DAYS_BIRTH_month', 'DAYS_BIRTH_week', 'DAYS_EMPLOYED_month', 'DAYS_EMPLOYED_week']\n","# n_best = 871\n","\n","for i_cv, (i_trn, i_val) in enumerate(cv.split(X, y)):\n","    # if i_cv == 0:\n","    #     clf = AutoLGB(objective='multiclass', metric='multi_logloss', params=params, \n","    #                   feature_selection=False, n_est=10000)\n","    #     clf.tune(X.iloc[i_trn], y[i_trn])\n","    #     n_best = clf.n_best\n","    #     features = clf.features\n","    #     params = clf.params\n","    #     print(f'best iteration: {n_best}')\n","    #     print(f'selected features ({len(features)}): {features}')        \n","    #     print(params)\n","    #     clf.fit(X.iloc[i_trn], y[i_trn])\n","    # else:\n","    #     train_data = lgb.Dataset(X[features].iloc[i_trn], label=y[i_trn])\n","    #     clf = lgb.train(params, train_data, n_best, verbose_eval=100)\n","    \n","    # p[i_val] = clf.predict(X[features].iloc[i_val])\n","    # p_tst += clf.predict(X_tst[features]) / n_fold\n","\n","    train_data = lgb.Dataset(X[features].iloc[i_trn], label=y[i_trn])\n","    clf = lgb.train(params, train_data, n_best, verbose_eval=100)\n","    \n","    p[i_val] = clf.predict(X[features].iloc[i_val])\n","    p_tst += clf.predict(X_tst[features]) / n_fold"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mALjiTojy5m7"},"source":["df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gye8JRxywDo2"},"source":["clf.feature_importance()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7iIrLDI3foq6"},"source":["from sklearn.metrics import log_loss\n","print(f'CV Log Loss: {log_loss(y, p):f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7LKB8UbJJYt"},"source":["submit = pd.read_csv(PATH+'sample_submission.csv')\n","\n","submit[submit.columns[1:]] = p_tst\n","# submit.to_csv('20210514_03_autolgb.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SP0NcexLsoL8"},"source":["def get_ans(df) :\n","    df.drop(['index'], axis=1, inplace=True)\n","    temp = pd.Series(df.apply(lambda x : 0 if x['0'] == max(x) else ( 1 if x['1'] == max(x) else 2),  axis=1))\n","    display(temp.value_counts()/len(temp))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmATy8ihTDO1"},"source":["get_ans(submit)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TmfKS59ZRM_T"},"source":["0.6667664914730588"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"itk2d0eM8qQL"},"source":["rank1 = pd.read_csv('/20210513_03_autolgb.csv').drop(['index'], axis=1)\n","rank2 = pd.read_csv('/20210513_submit_binary_autolgb.csv').drop(['index'], axis=1)\n","rank3 = pd.read_csv('/20210513_submit_autolgb.csv').drop(['index'], axis=1)\n","rank4 = pd.read_csv('/20210514_01_smote_autolgb.csv').drop(['index'], axis=1)\n","rank5 = pd.read_csv('/20210514_03_autolgb.csv').drop(['index'], axis=1)\n","rank6 = pd.read_csv('/20210514_02_autolgb.csv').drop(['index'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hxwx0mE91ZPd"},"source":["get_ans(submit.drop(['index'], axis=1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrXUDqwXv_k-"},"source":["2.0    0.641343  \n","1.0    0.236875  \n","0.0    0.121783  \n"]},{"cell_type":"code","metadata":{"id":"b-uKiD5dssn9"},"source":["get_ans(rank1), get_ans(rank2), get_ans(rank3), get_ans(rank4), get_ans(rank5), get_ans(rank6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpNebQ4gsC6f"},"source":[""],"execution_count":null,"outputs":[]}]}